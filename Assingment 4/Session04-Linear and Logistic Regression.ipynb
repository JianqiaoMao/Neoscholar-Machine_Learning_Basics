{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neoscholar Machine Learning Tutorials\n",
    "### Session 04. Linear and Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "1. Linear Regression\n",
    "2. Logistic Regression\n",
    "\n",
    "### Aim\n",
    "At the end of this session, you will be able to:\n",
    "- Implement your first Machine Learning model for regression and classification\n",
    "- Be more familiar with Sklearn lib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline\n",
    "1. Linear Regression\n",
    "    1.1 Basic Linear Regression\n",
    "    1.2 Advanced Linear Regression\n",
    "2. Logistics Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear Regression\n",
    "\n",
    "We are going to explore both the basic linear regression and more advanced linear regression with regulation terms, i.e., LASSO, Ridge, Elastic net regression. The modelling process begins from importing the dataset and ends at model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we are going to practice Linear Regression with Boston House Price Data that are already embedded in scikit-learn datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets as datasets\n",
    "BOSTON_DATA = datasets.load_boston()\n",
    "print(BOSTON_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check what features this dataset contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: print feature names\n",
    "print(BOSTON_DATA[None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we discussed before, EDA is one of the most important step to implement a machine learning model in practice. You have to not only understand the data you have but also clean it accordingly. In this tutorial, we will visualise the data and then analyse their correlations.\n",
    "\n",
    "First of all, let's define some useful funcitons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load both boston data and target, and convert it as dataframe.\n",
    "def add_target_to_data(dataset):\n",
    "    df = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
    "    print(\"Before adding target: \", df.shape)\n",
    "    df['PRICE'] = dataset.target\n",
    "    print(\"After adding target: {} \\n {}\\n\".format(df.shape, df.head(2)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualise the relations between features and the target\n",
    "def plotting_graph(df, features, n_row=2, n_col=5):\n",
    "    fig, axes = plt.subplots(n_row, n_col, figsize=(16, 8))\n",
    "    assert len(features) == n_row * n_col\n",
    "    for i, feature in enumerate(features):\n",
    "        row = int(i / n_col)\n",
    "        col = i % n_col\n",
    "        sns.regplot(x=feature, y='PRICE', data=df, ax=axes[row][col])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the function `add_target_to_data()` to transform the dataset into `Dataframe` type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_df = add_target_to_data(BOSTON_DATA)\n",
    "# TODO: print the first 5 samples of the dataset\n",
    "print(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only ten features are demonstrated for simplicity\n",
    "features = ['RM', 'ZN', 'INDUS', 'NOX', 'AGE', 'PTRATIO', 'LSTAT', 'RAD', 'CRIM', 'B']\n",
    "plotting_graph(boston_df, features, n_row=2, n_col=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation is a statistical measure that tells us about the association between the two variables. It describes how one variable behaves if there is some change in the other variable.\n",
    "\n",
    "#### Pearson vs Spearman correlation\n",
    "\n",
    "Both Pearson and Spearman are used for measuring the correlation but the difference between them lies in the kind of analysis we want.\n",
    "\n",
    "Pearson correlation: Pearson correlation evaluates the linear relationship between two continuous variables.\n",
    "\n",
    "Spearman correlation: Spearman correlation evaluates the monotonic relationship. The Spearman correlation coefficient is based on the ranked values for each variable rather than the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Pearson correlation matrix.\n",
    "correlation_matrix = boston_df.corr(method='pearson').round(2)\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise the correlation matrix by heat map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(correlation_matrix, cmap=\"YlGnBu\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Linear Regression\n",
    "\n",
    "#### Dataset Split\n",
    "\n",
    "We have practiced how to split a dataset into the testing and training set. Applying the `train_test_split()` function in `sklearn` to split your dataset with the ratio of 90:10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TODO: split your dataset into training and testing sets.\n",
    "# Set the random state as 17 to ensure every one to get the same result\n",
    "train_X, test_X, train_Y, test_Y = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further split your training set into training set and validation set with a ratio of 90:10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo: split your dataset into training and validation sets. \n",
    "# set the random state as 17 to ensure every one to get the same result\n",
    "train_X, val_X, train_Y, val_Y = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and evaluate the basic Linear Regression model\n",
    "\n",
    "Let's train your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Initialize your linear regression model\n",
    "lr_model = LinearRegression()\n",
    "# Train!\n",
    "lr_model.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we should evaluate it on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "# Make predictions with validation data!\n",
    "preds = lr_model.predict(val_X)\n",
    "# What is mse between the answer and your prediction?\n",
    "lr_mse = mean_squared_error(val_Y, preds)\n",
    "print('LR_MSE: {0:.4f}'.format(lr_mse))\n",
    "# Sort regression coefficient.\n",
    "coeff = pd.Series(data=lr_model.coef_, index=train_X.columns).sort_values(ascending=False)\n",
    "print(coeff)\n",
    "# Todo: Which feature is the most important?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the predicted price v.s. expected price (true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(val_Y, preds)\n",
    "plt.plot([0, 50], [0, 50], '--k')\n",
    "plt.xlabel('Expected price')\n",
    "plt.ylabel('Predicted price')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mannully implement the basic single variant linear regression\n",
    "\n",
    "Recall how we estimate the linear regression parameters based on OLS method. You should implement the `paramEstimates(x, y)` function that estimates the parameters of alpha and beta as follows:\n",
    "\\begin{align}\n",
    "\\hat{\\beta} & =  \\frac{\\sum_{i=1}^n x_i\\left(y_i - \\bar{y} \\right)}{\\sum_{i=1}^n x_i\\left(x_i - \\bar{x} \\right)}\\\\\n",
    "\\hat{\\alpha} & = \\bar{y}-\\hat{\\beta}\\bar{x}\n",
    "\\end{align}\n",
    "\n",
    "You have, however, to complete the `linearRegr_Predict(x_train, y_train, xTest)` function, or write your own, that returns the output variable y given the input x as follows: \n",
    "\\begin{align}\n",
    "\\hat{y} & = \\hat{\\alpha}+\\hat{\\beta}x\n",
    "\\end{align}\n",
    "\n",
    "For simplication, we only apply the most important feature `RM` as the regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly, let's implement it using sklearn\n",
    "# TODO: initialize your linear regression model\n",
    "singleVar_lr_model = None\n",
    "# TODO: Train!\n",
    "singleVar_lr_model.None\n",
    "print(\"The intercept (alpha) is {}:\".format(singleVar_lr_model.intercept_))\n",
    "print(\"The slope (beta) is {}:\".format(singleVar_lr_model.coef_[0]))\n",
    "# TODO: make predictions with validation data!\n",
    "singleVar_preds_sk = singleVar_lr_model.None\n",
    "# TODO: what is mse between the answer and your prediction?\n",
    "singleVar_lr_mse_sk = None\n",
    "print('LR_MSE_sk: {0:.4f}'.format(singleVar_lr_mse_sk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: code the function to estimate the parameters according to the above equations\n",
    "def paramEstimates(x, y):\n",
    "    beta = None\n",
    "    alpha = None\n",
    "    return alpha, beta\n",
    "\n",
    "def linearRegr_Predict(x_train, y_train,x_test):\n",
    "    # TODO: Estimate the parameter by calling the paramEstimates function\n",
    "    alpha, beta = None\n",
    "    print(\"The intercept (alpha) is: {}\".format(alpha))\n",
    "    print(\"The slope (beta) is: {}\".format(beta))\n",
    "    # TODO: Apply your estimated parameters to implement the linear regression model: y=a+bx\n",
    "    pred =  None\n",
    "    return pred\n",
    "\n",
    "singleVar_lr_preds_mannul=linearRegr_Predict(train_X.loc[:,'RM'], train_Y, val_X.loc[:,'RM'])\n",
    "\n",
    "# Now evaluate your model and compare the performance with the one using Sklearn lib\n",
    "singleVar_lr_mse_mannul = mean_squared_error(val_Y, preds_mannul)\n",
    "print('LR_MSE_mannul: {0:.4f}'.format(singleVar_lr_mse_mannul))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA\n",
    "\n",
    "Now we are going to compare the models performance if we apply the PCA to reduce the original dataset with 13 features into 8 dimensions.\n",
    "\n",
    "To keep the test data is unseen from the beginning to the end, you may need to fit your PCA model in training set and transform the testing set by applying the trained PCA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Todo: apply PCA to reduce the data dimensionality from 13 to 8\n",
    "pcaModel = None\n",
    "train_pca_X = pcaModel.None\n",
    "val_pca_X = pcaModel.transform(val_X)\n",
    "test_pca_X = pcaModel.None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a new model using the PCAed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model_pca = LinearRegression()\n",
    "lr_model_pca.fit(train_pca_X, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_pca = lr_model_pca.predict(val_pca_X)\n",
    "lr_pca_mse = mean_squared_error(val_Y, preds_pca)\n",
    "print('LR_PCA_MSE: {0:.4f}'.format(lr_pca_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(val_Y, preds_pca)\n",
    "plt.plot([0, 50], [0, 50], '--k')\n",
    "plt.xlabel('Expected price')\n",
    "plt.ylabel('Predicted price')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Linear Regression -  Ridge, Lasso and ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "models = {\n",
    "    \"Ridge\" : Ridge(),\n",
    "    \"Lasso\" : Lasso(),\n",
    "    \"ElasticNet\" : ElasticNet(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_record = {}\n",
    "for name, model in models.items():\n",
    "    curr_model = model\n",
    "    curr_model.fit(train_X, train_Y)\n",
    "    preds = curr_model.predict(val_X)\n",
    "    mse = mean_squared_error(val_Y, preds)\n",
    "    print('{} MSE: {}'.format(name, mse))\n",
    "    # Record predictions for every model\n",
    "    pred_record.update({name : preds})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compare these models' performance visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = models.keys()\n",
    "fig = plt.figure(figsize=(6, 8))\n",
    "i=1\n",
    "for model in model_names:\n",
    "    prediction = pred_record[model]\n",
    "    plt.subplot(310+i)\n",
    "    plt.scatter(val_Y, prediction)\n",
    "    plt.plot([0, 50], [0, 50], '--k')\n",
    "    plt.xlabel('Expected price')\n",
    "    plt.ylabel('Predicted price')\n",
    "    plt.tight_layout()\n",
    "    plt.title(model)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression\n",
    "\n",
    "Useful videos:\n",
    "1. [Andrew Ng's explanation 1](https://www.youtube.com/watch?v=-la3q9d7AKQ)\n",
    "2. [Andrew Ng's explanation 2](https://www.youtube.com/watch?v=t1IT5hZfS48)\n",
    "3. [Andrew Ng's explanation 3](https://www.youtube.com/watch?v=F_VG4LNjZZw)\n",
    "4. [Andrew Ng's explanation 4](https://www.youtube.com/watch?v=HIQlmHxI6-0)\n",
    "\n",
    "Logistic regression is a well-motivated approach to discriminative classification which leads to a smooth, convex, optimisation problem.  \n",
    "\n",
    "Logistic regression is also a basis of Neural Network. Logistic Regression is sometimes called, a single node of Artificial Neuron. We will get back to what this means afterwards when we are doing Deep Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In which case do we use classification?\n",
    "\n",
    "Let's firstly generate a toy dataset that is suitable for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateDataset(seed=0):\n",
    "    np.random.seed(seed)\n",
    "    n_samples= 100\n",
    "\n",
    "    X = np.random.normal(size=n_samples)\n",
    "    y = (X > 0).astype(np.float)\n",
    "\n",
    "    X[X > 0] *= 5\n",
    "    X += .7 * np.random.normal(size=n_samples)\n",
    "    X = X[:, np.newaxis]\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = generateDataset(seed=0)\n",
    "plt.scatter(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if our data looks like the above? Would you still use your linear regression model?  \n",
    "Probably not. When your data has classes and your task is to classify the data, you normally use classification method, and Logistic Regression is a good start in learning classification.  \n",
    "Please do watch the Andrew Ng's video on Logistic Regression to fully understand mathematically.  \n",
    "\n",
    "Plus, note the the term 'logistic regression' has a word 'regression' inside.  \n",
    "It is because the logistic regression is a generalised linear model using the same basic formula of linear regression but it is regressing for the probability of a categorical outcome by using `sigmoid` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# TODO: initialize your linear regression model\n",
    "logistic_clf = None\n",
    "# TODO: Train!\n",
    "logistic_clf.None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's generate another group of data to evaluate (test) our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "\n",
    "X_test, y_test = generateDataset(seed=45)\n",
    "# Generate predictions on the test set\n",
    "preds_logistic = logistic_clf.None\n",
    "# Evaluate the model\n",
    "print('Accuracy on test set: '+str(accuracy_score(y_test,preds_logistic)))\n",
    "print(classification_report(y_test,preds_logistic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function called `compare_logistic_linear` fits the data into the logistic regression model and a simple ordinary least squared linear regression model. Then, it plots the two in one plot for better visual representation on why you should consider using classification rather than regression.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "\n",
    "def compare_logistic_linear(model, X_data, y_data):\n",
    "    \"\"\"\n",
    "    This function plots the given data - X_data and y_data\n",
    "    then fit the data both into given `model` and LinearRegression model.\n",
    "    Then shows the difference by plotting both of them.\n",
    "    \"\"\"\n",
    "    plt.clf()\n",
    "    plt.scatter(X_data.ravel(), y_data, color='black', zorder=20)\n",
    "    X_test = np.linspace(-5, 10, 300)\n",
    "\n",
    "    loss = expit(X_test * model.coef_ + model.intercept_).ravel()\n",
    "    plt.plot(X_test, loss, color='red', linewidth=3)\n",
    "    \n",
    "    # Ordinary Least Squared Linear Regression\n",
    "    ols = LinearRegression()\n",
    "    ols.fit(X_data, y_data)\n",
    "    plt.plot(X_test, ols.coef_ * X_test + ols.intercept_, linewidth=1)\n",
    "    plt.axhline(.5, color='.5')\n",
    "\n",
    "    plt.ylabel('y')\n",
    "    plt.xlabel('X')\n",
    "    plt.xticks(range(-5, 10))\n",
    "    plt.yticks([0, 0.5, 1])\n",
    "    plt.ylim(-.25, 1.25)\n",
    "    plt.xlim(-4, 10)\n",
    "    plt.legend(('Logistic Regression Model', 'Linear Regression Model'),\n",
    "               loc=\"lower right\", fontsize='small')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_logistic_linear(logistic_clf, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
